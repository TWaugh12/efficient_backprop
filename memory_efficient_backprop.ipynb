{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwDdnUOMnpblhzU1Niq406"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "4j2qTPv5ckCl"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade pip\n",
        "!pip install torch\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "from collections import defaultdict\n",
        "\n",
        "# Main implementation with forward and backward passes\n",
        "\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, weight, bias, labels, vocab_chunk_size: int, logit_transform=None):\n",
        "        \"\"\"\n",
        "        Computes memoryâ€efficient cross entropy loss.\n",
        "        \"\"\"\n",
        "        ctx.logit_transform = logit_transform\n",
        "        original_shape = X.shape\n",
        "        if X.dim() > 2:\n",
        "            X_flat = X.view(-1, X.shape[-1])\n",
        "            labels_flat = labels.view(-1)\n",
        "        else:\n",
        "            X_flat = X\n",
        "            labels_flat = labels\n",
        "\n",
        "\n",
        "        N, hidden = X_flat.shape\n",
        "        vocab = weight.shape[0]\n",
        "        device, dtype = X.device, X.dtype\n",
        "        global_max = torch.full((N,), -float('inf'), device=device, dtype=dtype)\n",
        "        ground_truth = torch.empty(N, device=device, dtype=dtype)\n",
        "\n",
        "        for j in range(0, vocab, vocab_chunk_size):\n",
        "            j_end = min(j + vocab_chunk_size, vocab)\n",
        "            z_chunk = X_flat @ weight[j:j_end].t()\n",
        "            if bias is not None:\n",
        "                z_chunk = z_chunk + bias[j:j_end].unsqueeze(0)\n",
        "            if logit_transform is not None:\n",
        "                tz_chunk = logit_transform(z_chunk)\n",
        "            else:\n",
        "                tz_chunk = z_chunk\n",
        "\n",
        "            global_max = torch.maximum(global_max, tz_chunk.max(dim=1)[0])\n",
        "            mask = (labels_flat >= j) & (labels_flat < j_end)\n",
        "            if mask.any():\n",
        "                ground_truth[mask] = tz_chunk[mask, labels_flat[mask] - j]\n",
        "\n",
        "        sum_exp = torch.zeros(N, device=device, dtype=dtype)\n",
        "        for j in range(0, vocab, vocab_chunk_size):\n",
        "            j_end = min(j + vocab_chunk_size, vocab)\n",
        "            z_chunk = X_flat @ weight[j:j_end].t()\n",
        "            if bias is not None:\n",
        "                z_chunk = z_chunk + bias[j:j_end].unsqueeze(0)\n",
        "            if logit_transform is not None:\n",
        "                tz_chunk = logit_transform(z_chunk)\n",
        "            else:\n",
        "                tz_chunk = z_chunk\n",
        "            sum_exp += torch.exp(tz_chunk - global_max.unsqueeze(1)).sum(dim=1)\n",
        "\n",
        "        loss = (global_max + torch.log(sum_exp + 1e-12) - ground_truth).mean()\n",
        "\n",
        "        ctx.save_for_backward(X_flat, weight, bias, labels_flat, global_max, sum_exp)\n",
        "        ctx.vocab_chunk_size = vocab_chunk_size\n",
        "        ctx.N = N\n",
        "        ctx.hidden = hidden\n",
        "        ctx.original_shape = original_shape\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        X_flat, weight, bias, labels_flat, global_max, sum_exp = ctx.saved_tensors\n",
        "        vocab_chunk_size = ctx.vocab_chunk_size\n",
        "        N, hidden = ctx.N, ctx.hidden\n",
        "        vocab = weight.shape[0]\n",
        "        device, dtype = X_flat.device, X_flat.dtype\n",
        "        logit_transform = ctx.logit_transform\n",
        "\n",
        "        grad_X = torch.zeros_like(X_flat)\n",
        "        grad_W = torch.zeros_like(weight)\n",
        "        grad_b = torch.zeros_like(bias) if bias is not None else None\n",
        "\n",
        "        scale = grad_output / N\n",
        "\n",
        "        for j in range(0, vocab, vocab_chunk_size):\n",
        "            j_end = min(j + vocab_chunk_size, vocab)\n",
        "            z_chunk = X_flat @ weight[j:j_end].t()\n",
        "            if bias is not None:\n",
        "                z_chunk = z_chunk + bias[j:j_end].unsqueeze(0)\n",
        "\n",
        "            if logit_transform is not None:\n",
        "                with torch.enable_grad():\n",
        "                    z_chunk = z_chunk.detach().clone().requires_grad_(True)\n",
        "                    tz_chunk = logit_transform(z_chunk)\n",
        "                    dummy = tz_chunk.sum()\n",
        "                    dtz_dz = torch.autograd.grad(dummy, z_chunk, create_graph=True)[0]\n",
        "                current_tz = tz_chunk.detach()\n",
        "            else:\n",
        "                current_tz = z_chunk\n",
        "                dtz_dz = None\n",
        "\n",
        "            logits_chunk_stable = current_tz - global_max.unsqueeze(1)\n",
        "            exp_chunk = torch.exp(logits_chunk_stable)\n",
        "            p = exp_chunk / sum_exp.unsqueeze(1)\n",
        "\n",
        "            mask = (labels_flat >= j) & (labels_flat < j_end)\n",
        "            if mask.any():\n",
        "                indices = (labels_flat[mask] - j).unsqueeze(1)\n",
        "\n",
        "                p[mask].scatter_(1, indices, p[mask].gather(1, indices) - 1)\n",
        "\n",
        "            dlogits = p * scale\n",
        "            if logit_transform is not None:\n",
        "                dlogits_effective = dlogits * dtz_dz\n",
        "            else:\n",
        "                dlogits_effective = dlogits\n",
        "\n",
        "            # Accumulate gradients.\n",
        "            grad_X += dlogits_effective @ weight[j:j_end]\n",
        "            grad_W[j:j_end] += dlogits_effective.t() @ X_flat\n",
        "            if grad_b is not None:\n",
        "                grad_b[j:j_end] += dlogits_effective.sum(dim=0)\n",
        "\n",
        "        grad_X = grad_X.view(ctx.original_shape)\n",
        "        return grad_X, grad_W, grad_b, None, None, None\n",
        "\n",
        "\n",
        "# from intermeditate tensors for autograd\n",
        "def memory_efficient_loss(X, linear, labels, batch_chunk_size=None, vocab_chunk_size=1024, logit_transform=None):\n",
        "    return MemoryEfficientLinear.apply(X, linear.weight, linear.bias, labels, vocab_chunk_size, logit_transform)"
      ],
      "metadata": {
        "id": "wV5sOAUqcqM1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Test code ###\n",
        "def baseline_loss(X, linear, labels):\n",
        "    if X.dim() == 3:\n",
        "        X = X.view(-1, X.shape[-1])  # B * seq_len, hidden_dim\n",
        "        labels = labels.view(-1)      # B *seq_len\n",
        "    logits = linear(X)  # B * seq_len, vocab_size\n",
        "    if linear.bias is not None:\n",
        "        logits = logits + linear.bias\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "    loss = loss_fn(logits, labels)\n",
        "    loss.backward()\n",
        "    return loss\n",
        "\n",
        "def _test_vram_reduction(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device):\n",
        "    print(\"\\n--- Test: VRAM Reduction ---\")\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "    base_memory = torch.cuda.memory_allocated()\n",
        "    print(f\"Base memory: {base_memory/1024**2:.2f} MB\")\n",
        "\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear_standard = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "    loss_standard = baseline_loss(X.clone(), linear_standard, labels)\n",
        "    vram_standard = torch.cuda.max_memory_allocated() - base_memory\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.synchronize()\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear_efficient = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "    loss_efficient = memory_efficient_loss(X.clone(), linear_efficient, labels, batch_chunk_size=batch_chunk_size, vocab_chunk_size=vocab_chunk_size)\n",
        "    loss_efficient.backward()\n",
        "    vram_efficient = torch.cuda.max_memory_allocated() - base_memory\n",
        "\n",
        "    reduction = (1 - (vram_efficient / vram_standard)) * 100\n",
        "    passed = reduction >= 40\n",
        "    print(f\"Standard VRAM delta: {vram_standard/1024**2:.2f} MB\")\n",
        "    print(f\"Efficient VRAM delta: {vram_efficient/1024**2:.2f} MB\")\n",
        "    print(f\"VRAM Reduction: {reduction:.2f}%  ->  {'PASS' if passed else 'FAIL'}\")\n",
        "    return passed, {\"vram_standard_mb\": vram_standard/1024**2, \"vram_efficient_mb\": vram_efficient/1024**2, \"vr_reduction_percent\": reduction}\n",
        "\n",
        "def _test_numerical_correctness(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device):\n",
        "    print(\"\\n--- Test: Numerical Correctness ---\")\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear_standard = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "    linear_efficient = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        linear_efficient.weight.copy_(linear_standard.weight)\n",
        "        if linear_standard.bias is not None:\n",
        "            linear_efficient.bias.copy_(linear_standard.bias)\n",
        "\n",
        "    loss_standard = baseline_loss(X.clone(), linear_standard, labels)\n",
        "    grad_standard = linear_standard.weight.grad.clone()\n",
        "\n",
        "    linear_standard.zero_grad()\n",
        "    loss_efficient = memory_efficient_loss(X.clone(), linear_efficient, labels, batch_chunk_size=batch_chunk_size, vocab_chunk_size=vocab_chunk_size)\n",
        "    loss_efficient.backward()\n",
        "    grad_efficient = linear_efficient.weight.grad.clone()\n",
        "\n",
        "    loss_close = torch.allclose(loss_standard, loss_efficient, rtol=1e-1, atol=1e-1)\n",
        "    grad_close = torch.allclose(grad_standard, grad_efficient, rtol=1e-1, atol=1e-1)\n",
        "    print(f\"Loss match: {loss_close}\")\n",
        "    print(f\"Gradient match: {grad_close}\")\n",
        "    passed = loss_close and grad_close\n",
        "    return passed, {\"loss_standard\": loss_standard.item(), \"loss_efficient\": loss_efficient.item()}\n",
        "\n",
        "def _test_dtype_handling(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device):\n",
        "    print(\"\\n--- Test: Data Type Handling ---\")\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear_efficient = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "    loss = memory_efficient_loss(X, linear_efficient, labels, batch_chunk_size=batch_chunk_size, vocab_chunk_size=vocab_chunk_size)\n",
        "    loss.backward()\n",
        "    input_dtype_correct = (X.dtype == dtype)\n",
        "    weight_dtype_correct = (linear_efficient.weight.dtype == dtype)\n",
        "    bias_dtype_correct = ((linear_efficient.bias.dtype == dtype) if linear_efficient.bias is not None else True)\n",
        "    passed = input_dtype_correct and weight_dtype_correct and bias_dtype_correct\n",
        "    print(f\"Input dtype: {X.dtype} (expected: {dtype})\")\n",
        "    print(f\"Weight dtype: {linear_efficient.weight.dtype} (expected: {dtype})\")\n",
        "    print(f\"Bias dtype: {linear_efficient.bias.dtype if linear_efficient.bias is not None else 'N/A'} (expected: {dtype})\")\n",
        "    return passed, {\"input_dtype\": str(X.dtype), \"weight_dtype\": str(linear_efficient.weight.dtype), \"bias_dtype\": str(linear_efficient.bias.dtype)}\n",
        "\n",
        "def _test_show_ce_loss_works(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device):\n",
        "    print(\"\\n--- Test: Show CE Loss Works ---\")\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear_standard = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "    linear_efficient = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        linear_efficient.weight.copy_(linear_standard.weight)\n",
        "        if linear_standard.bias is not None:\n",
        "            linear_efficient.bias.copy_(linear_standard.bias)\n",
        "\n",
        "    loss_standard = baseline_loss(X.clone(), linear_standard, labels).item()\n",
        "    linear_standard.zero_grad()\n",
        "    loss_efficient = memory_efficient_loss(X.clone(), linear_efficient, labels, batch_chunk_size=batch_chunk_size, vocab_chunk_size=vocab_chunk_size).item()\n",
        "    linear_efficient.zero_grad()\n",
        "    loss_close = torch.isclose(torch.tensor(loss_standard), torch.tensor(loss_efficient), rtol=1e-2, atol=1e-2)\n",
        "    print(f\"Standard CE Loss: {loss_standard:.4f}\")\n",
        "    print(f\"Efficient CE Loss: {loss_efficient:.4f}\")\n",
        "    print(f\"CE Loss match: {loss_close}\")\n",
        "    return loss_close, {\"loss_standard\": loss_standard, \"loss_efficient\": loss_efficient}\n",
        "\n",
        "def _test_show_other_functions_work(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device):\n",
        "    print(\"\\n--- Test: Show Other Functions Work ---\")\n",
        "\n",
        "    def dummy_transform(z):\n",
        "        return torch.log1p(torch.exp(z))\n",
        "\n",
        "    def dummy_loss(X, linear, labels, vocab_chunk_size=128):\n",
        "        if X.dim() == 3:\n",
        "            X = X.view(-1, X.shape[-1])\n",
        "            labels = labels.view(-1)\n",
        "        logits = linear(X)\n",
        "        transformed = torch.log1p(torch.exp(logits))\n",
        "        loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "        return loss_fn(transformed, labels)\n",
        "\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear_standard = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "    linear_efficient = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        linear_efficient.weight.copy_(linear_standard.weight)\n",
        "        if linear_standard.bias is not None:\n",
        "            linear_efficient.bias.copy_(linear_standard.bias)\n",
        "\n",
        "    loss_standard = dummy_loss(X.clone(), linear_standard, labels, vocab_chunk_size).item()\n",
        "    linear_standard.zero_grad()\n",
        "    loss_efficient = memory_efficient_loss(\n",
        "        X.clone(),\n",
        "        linear_efficient,\n",
        "        labels,\n",
        "        batch_chunk_size=batch_chunk_size,\n",
        "        vocab_chunk_size=vocab_chunk_size,\n",
        "        logit_transform=dummy_transform\n",
        "    ).item()\n",
        "    linear_efficient.zero_grad()\n",
        "    loss_close = torch.isclose(torch.tensor(loss_standard), torch.tensor(loss_efficient), rtol=1e-2, atol=1e-2)\n",
        "    print(f\"Dummy Loss Standard: {loss_standard:.4f}\")\n",
        "    print(f\"Dummy Loss Efficient: {loss_efficient:.4f}\")\n",
        "    print(f\"Dummy Loss match: {loss_close}\")\n",
        "    return loss_close, {\"loss_standard\": loss_standard, \"loss_efficient\": loss_efficient}\n",
        "\n",
        "def _test_dynamic_chunk_sizes(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, device):\n",
        "    print(\"\\n--- Test: Dynamic Chunk Sizes ---\")\n",
        "    chunk_size_pairs = [\n",
        "        (32, 64),\n",
        "        (128, 256),\n",
        "        (256, 512),\n",
        "    ]\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "\n",
        "    base_loss = None\n",
        "    all_losses_match = True\n",
        "    for vocab_chunk, batch_chunk in chunk_size_pairs:\n",
        "        print(f\"\\nTesting with vocab_chunk_size={vocab_chunk}, batch_chunk_size={batch_chunk}\")\n",
        "        loss = memory_efficient_loss(\n",
        "            X.clone(),\n",
        "            linear,\n",
        "            labels,\n",
        "            batch_chunk_size=batch_chunk,\n",
        "            vocab_chunk_size=vocab_chunk\n",
        "        )\n",
        "        if base_loss is None:\n",
        "            base_loss = loss.item()\n",
        "        else:\n",
        "            loss_matches = torch.isclose(torch.tensor(loss.item()), torch.tensor(base_loss), rtol=1e-2, atol=1e-2)\n",
        "            if not loss_matches:\n",
        "                all_losses_match = False\n",
        "                print(f\"Loss mismatch: {loss.item()} vs base {base_loss}\")\n",
        "        del loss\n",
        "        torch.cuda.empty_cache()\n",
        "    print(f\"All chunk sizes produce consistent results: {'PASS' if all_losses_match else 'FAIL'}\")\n",
        "    return all_losses_match, {\"dynamic_chunks_work\": all_losses_match}\n",
        "\n",
        "def _test_hardcoded_gradients(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device):\n",
        "    print(\"\\n--- Test: Hardcoded Gradients Check ---\")\n",
        "    X = torch.randn(batch_size, query_length, hidden_dimension, device=device, dtype=dtype, requires_grad=True)\n",
        "    labels = torch.randint(0, vocabulary_size, (batch_size, query_length), device=device)\n",
        "    linear_efficient = nn.Linear(hidden_dimension, vocabulary_size, bias=True).to(device).to(dtype)\n",
        "    loss = memory_efficient_loss(X.clone(), linear_efficient, labels, batch_chunk_size=batch_chunk_size, vocab_chunk_size=vocab_chunk_size)\n",
        "    loss.backward()\n",
        "    weight_grad_zero = torch.all(linear_efficient.weight.grad == 0)\n",
        "    bias_grad_zero = torch.all(linear_efficient.bias.grad == 0) if linear_efficient.bias is not None else False\n",
        "    hardcoded = weight_grad_zero or bias_grad_zero\n",
        "    print(f\"Weight grad all zero: {weight_grad_zero}\")\n",
        "    print(f\"Bias grad all zero: {bias_grad_zero}\")\n",
        "    passed = not hardcoded\n",
        "    return passed, {\"hardcoded_gradients\": hardcoded}\n",
        "\n",
        "def test_suite(dtype=torch.float16,\n",
        "               batch_size=8,\n",
        "               query_length=2048,\n",
        "               hidden_dimension=2048,\n",
        "               vocabulary_size=32000,\n",
        "               vocab_chunk_size=128,\n",
        "               batch_chunk_size=64,\n",
        "               device=\"cuda\",\n",
        "               epochs=1,\n",
        "               steps_per_epoch=10):\n",
        "\n",
        "    test_results = defaultdict(lambda: {\"result\": False, \"stats\": {}})\n",
        "    if device == \"cuda\" and not torch.cuda.is_available():\n",
        "        print(\"CUDA not available, switching to CPU.\")\n",
        "        device = \"cpu\"\n",
        "\n",
        "    print(\"\\n=== Running Streamlined Test Suite ===\")\n",
        "    print(f\"Device: {device}, dtype: {dtype}\")\n",
        "    print(f\"batch_size: {batch_size}, query_length: {query_length}, hidden_dimension: {hidden_dimension}, vocabulary_size: {vocabulary_size}\")\n",
        "    print(f\"vocab_chunk_size: {vocab_chunk_size}, batch_chunk_size: {batch_chunk_size}\")\n",
        "\n",
        "    E_score = 0\n",
        "\n",
        "    passed, stats = _test_vram_reduction(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device)\n",
        "    test_results[\"VRAM Reduction\"] = {\"result\": passed, \"stats\": stats}\n",
        "    print(f\"VRAM Reduction Test: {'PASS' if passed else 'FAIL'}\")\n",
        "    if passed: E_score += 2\n",
        "\n",
        "    passed, stats = _test_numerical_correctness(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device)\n",
        "    test_results[\"Numerical Correctness\"] = {\"result\": passed, \"stats\": stats}\n",
        "    print(f\"Numerical Correctness Test: {'PASS' if passed else 'FAIL'}\")\n",
        "    if passed: E_score += 0\n",
        "\n",
        "    passed, stats = _test_dtype_handling(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device)\n",
        "    test_results[\"Data Type Handling\"] = {\"result\": passed, \"stats\": stats}\n",
        "    print(f\"Data Type Handling Test: {'PASS' if passed else 'FAIL'}\")\n",
        "    if not passed:\n",
        "        print(\"Data type handling failed. E_score set to 0.\")\n",
        "        E_score = 0\n",
        "\n",
        "    passed, stats = _test_show_ce_loss_works(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device)\n",
        "    test_results[\"CE Loss Works\"] = {\"result\": passed, \"stats\": stats}\n",
        "    print(f\"CE Loss Works Test: {'PASS' if passed else 'FAIL'}\")\n",
        "    if passed: E_score += 1\n",
        "\n",
        "    passed, stats = _test_show_other_functions_work(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device)\n",
        "    test_results[\"Other Functions Work\"] = {\"result\": passed, \"stats\": stats}\n",
        "    print(f\"Other Functions Work Test: {'PASS' if passed else 'FAIL'}\")\n",
        "    if passed: E_score += 1\n",
        "\n",
        "    passed, stats = _test_hardcoded_gradients(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, vocab_chunk_size, batch_chunk_size, device)\n",
        "    test_results[\"Hardcoded Gradients\"] = {\"result\": passed, \"stats\": stats}\n",
        "    print(f\"Hardcoded Gradients Test: {'PASS' if passed else 'FAIL'}\")\n",
        "    if not passed:\n",
        "        print(\"Detected hardcoded gradients. E_score set to 0.\")\n",
        "        E_score = 0\n",
        "\n",
        "    passed, stats = _test_dynamic_chunk_sizes(dtype, batch_size, query_length, hidden_dimension, vocabulary_size, device)\n",
        "    test_results[\"Dynamic Chunk Sizes\"] = {\"result\": passed, \"stats\": stats}\n",
        "    print(f\"Dynamic Chunk Sizes Test: {'PASS' if passed else 'FAIL'}\")\n",
        "    if passed:\n",
        "        E_score += 1\n",
        "        test_results[\"Total Score\"] = {\"result\": (E_score == 10), \"stats\": {\"E_score\": E_score}}\n",
        "        print(f\"\\n=== Test Suite Finished ===\")\n",
        "        print(f\"Total Score: {E_score} / 10\")\n",
        "    else:\n",
        "        test_results[\"Total Score\"] = {\"result\": (E_score == 10), \"stats\": {\"E_score\": E_score}}\n",
        "        print(f\"\\n=== Test Suite Finished ===\")\n",
        "        print(f\"Total Score: {E_score} / 10\")\n",
        "    return test_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = test_suite(\n",
        "        dtype=torch.float16,\n",
        "        batch_size=8,\n",
        "        query_length=2048,\n",
        "        hidden_dimension=2048,\n",
        "        vocabulary_size=32000,\n",
        "        vocab_chunk_size=128,\n",
        "        batch_chunk_size=64,\n",
        "        device=\"cuda\",\n",
        "        epochs=1,\n",
        "        steps_per_epoch=10\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R18wvlPJdA_L",
        "outputId": "a7f49ab4-e672-4b74-a4f3-66607cf56c05"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Streamlined Test Suite ===\n",
            "Device: cuda, dtype: torch.float16\n",
            "batch_size: 8, query_length: 2048, hidden_dimension: 2048, vocabulary_size: 32000\n",
            "vocab_chunk_size: 128, batch_chunk_size: 64\n",
            "\n",
            "--- Test: VRAM Reduction ---\n",
            "Base memory: 0.00 MB\n",
            "Standard VRAM delta: 4262.31 MB\n",
            "Efficient VRAM delta: 921.70 MB\n",
            "VRAM Reduction: 78.38%  ->  PASS\n",
            "VRAM Reduction Test: PASS\n",
            "\n",
            "--- Test: Numerical Correctness ---\n",
            "Loss match: True\n",
            "Gradient match: True\n",
            "Numerical Correctness Test: PASS\n",
            "\n",
            "--- Test: Data Type Handling ---\n",
            "Input dtype: torch.float16 (expected: torch.float16)\n",
            "Weight dtype: torch.float16 (expected: torch.float16)\n",
            "Bias dtype: torch.float16 (expected: torch.float16)\n",
            "Data Type Handling Test: PASS\n",
            "\n",
            "--- Test: Show CE Loss Works ---\n",
            "Standard CE Loss: 10.5469\n",
            "Efficient CE Loss: 10.5469\n",
            "CE Loss match: True\n",
            "CE Loss Works Test: PASS\n",
            "\n",
            "--- Test: Show Other Functions Work ---\n",
            "Dummy Loss Standard: 10.4141\n",
            "Dummy Loss Efficient: 10.4141\n",
            "Dummy Loss match: True\n",
            "Other Functions Work Test: PASS\n",
            "\n",
            "--- Test: Hardcoded Gradients Check ---\n",
            "Weight grad all zero: False\n",
            "Bias grad all zero: False\n",
            "Hardcoded Gradients Test: PASS\n",
            "\n",
            "--- Test: Dynamic Chunk Sizes ---\n",
            "\n",
            "Testing with vocab_chunk_size=32, batch_chunk_size=64\n",
            "\n",
            "Testing with vocab_chunk_size=128, batch_chunk_size=256\n",
            "\n",
            "Testing with vocab_chunk_size=256, batch_chunk_size=512\n",
            "All chunk sizes produce consistent results: PASS\n",
            "Dynamic Chunk Sizes Test: PASS\n",
            "\n",
            "=== Test Suite Finished ===\n",
            "Total Score: 5 / 10\n"
          ]
        }
      ]
    }
  ]
}